<h1>深度学习基础</h1><br>

<h2>深度强化学习</h2><br>

**训练**：基于**大量样本**进行算法迭代更新达到最优的过程
**强化学习的训练**：在交互中产生样本，产生样本→算法更新→再次产生样本→再次算法更新的动态循环过程，而不是准备样本，算法更新的静态训练过程
**深度强化学习的充分性**：深度学习是用来提高强化学习中预测的效果，如Q表可以用神经网络拟合，但不是强化学习的必要条件，一些传统的预测模型（决策树，贝叶斯模型）也可以使用

**强化学习的反馈系统机制**：强化学习相当于在深度学习的基础上增加了一条回路，与环境交互产生反馈信息，训练出更好的模型

![alt text](https://datawhalechina.github.io/joyrl-book/figs/ch6/dl_rl.png)

<h2>线性回归模型</h2><br>

**模型输入**：$x=[x_1,x_2,\cdots,x_m]$
**模型参数**：$ω ,b$
**模型输出**：$f(x;ω;b)=ω_1x_1+ω_2x_2,\cdots,ω_mx_m+b=ω^Tx+b=f^θ(x)=θ^Tx$
**拟合问题**：求得一组最优的参数$θ^⋆$,使得模型能正确的预测
**模型**：通过$f^θ(x)=θ^Tx$来拟合一组散点，这条曲线f就被称为模型。用来拟合的散点被称为**样本**，大量的样本构成**训练集**

**没有免费午餐定理(NoFreeLunch Therom)**:没有一种算法完全优于另外一种算法，一种算法在一方面比另外一种算法优越，再另外一个方面要更劣势
**过拟合**：在训练集上表现很好，在测试集上表现很差
**欠拟合**：在训练集上表现很差，在测试集上表现较好

<h2>梯度下降</h2><br>

**基本思想**
初始化参数：选择一个初始点或参数的初始值
计算梯度：在当前点计算函数的梯度即偏导数。梯度指向函数值增加最快的方向
更新参数：按照负梯度方向更新参数，可以减少函数值。通常用bp算法实现
重复以上步骤直到梯度趋近于0

**能够基于任何可导的函数求解**，是一种基于贪心思想的方法

**梯度方向**：（偏导数定义）梯度下降最快的方向
**学习率**：按照梯度方向下降的步长
**batch**：每次梯度下降的迭代过程，选取一个小批量的样本来计算梯度，这个小批量的样本被称为一个batch
**局部最优解**：小batch导致陷入局部的极小值而无法达到全局最小值，batch太大则需要更多的计算资源，需要根据实际情况来选择一个合适的batch大小

![alt text](https://datawhalechina.github.io/joyrl-book/figs/ch6/gd.png)
**优化器(optimizer)**:加入动量，Adam等对梯度下降的机制处理
**随机梯度下降**：单纯的梯度下降按照原本样本的顺序不断迭代去拟合模型参数，随机梯度下降则是随机抽取样本。利用随机性可能帮助算法跳出一些局部最优解，从而使得算法的收敛性更高，增强鲁棒性。
**批量梯度下降**：使用整个训练样本来迭代，batch很大，**迭代方向比较准确但计算开销大**
**小批量梯度下降(mini-batch gradient descent)**:使用一小部分样本来迭代，batch很小，**计算开销比较小，但迭代的方向不准确**

**小批量随机梯度下降(mini-batch stochastic gradient descent)**:可以兼顾到所有优点，**训练更加稳定，算法效果更好**

<h2>逻辑回归</h2><br>

**sigmoid函数**：

$$sigmoid(z)=\frac{1}{1+\exp{(-z)}}$$

sigmoid 函数可以将输入的任意实数映射到(0,1)区间内，对其输出值进行判断，小于0.5认为是类别0，反之为类别1。

**交叉熵损失**：线性回归函数的损失函数是均方差函数，而逻辑回归函数一般是交叉熵损失

**逻辑回归的优缺点**：增加了模型的非线性能力，但只能解决二分类问题。二分类组合多分类问题：sigmoid→softmax

<h2>全连接网络</h2><br>

![alt text](https://datawhalechina.github.io/joyrl-book/figs/ch6/ann_vs_dnn.png)

将人工神经网络的线性层堆叠起来，前一层的神经元的输出都会输入到下一层的所有神经元中，这样就可以得到一个全链接网络。


<p align="center">
 <img src="https://datawhalechina.github.io/joyrl-book/figs/ch6/mlp.png" width="300" >
</p>

这样的网络被称为**全连接网络(fully connected network)**,也称**多层感知机(multi-layer perception,MLP)**,他是最基础的深度神经网络模型。

**定义式**：$$x^l=σ(z),z=Wx^{l-1}+b=θx^{l-1}$$

其中$W∈R^{d^{l-1}×d^l}$ 为**权重矩阵**，b为**偏置矩阵**,这两个矩阵通常被看做一个参数θ，σ(⋅)称为**激活函数**

**激活函数**:除sigmoid之外，还有softmax，ReLU和tanh等，ReLU函数将神经元输出映射到(0,1)之间，后者则映射到(-1,1)之间，映射函数的选择需要根据具体的问题来决定。

一个$l$层的神经网络模型

$$\begin{aligned}
    x^{(1)}=&σ_1(W^{(1)}x^{(0)}+b^{(1)})\\
    x^{(2)}=&σ_2(W^{(2)}x^{(2)}+b^{(2)})\\
    &\vdots \\ 
    x^{(l)}=&σ_l(W^{(l)}x^{(l)}+b^{(l)})
\end{aligned}$$

其中$θ=\{W^{(1)},b^{(1)},\cdots,W^{(l)},b^{(l)}\}$
寻找一组θ使得神经网络模型输出的尽可能接近真实值的过程就是神经网络的训练过程，同基础的线性模型类似，神经网络也可以通过梯度下降的方法来求解最优解。

<h2>更高级的神经网络</h2><br>

<h3>卷积神经网络</h2><br>


**卷积神经网络(convolutional-neural-network)** 适合处理具有网格结构的数据，具有以下几个特点

局部感受野：只与前一层的一个小区域节点相连，可以减少参数数量，并使得网络能专注于捕捉局部特征
权重共享：同一层的不同位置卷积核的权重是共享的，不仅大大减少了参数数量，还能在不同位置检测同样的特征
池化层：插入在连续的卷积层之间，减少特征图的尺寸，减少参数数量并提高网络的计算效率，常见的池化操作是最大池化(Max-Pooling),它将输入特征图划分为若干个小区域，并输出每个区域的最大值
归一化和Dropout:为了优化网络和防止性能过剩，可以在网络中添加归一化层(batch Normalization)和 Dropout。